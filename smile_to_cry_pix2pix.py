import torch
import torch.nn as nn
from diffusers import UNet2DModel, DDPMScheduler, DDIMScheduler
from torchvision import transforms
from torch.utils.data import DataLoader, Dataset
from tqdm import tqdm
from torchvision.utils import save_image
from PIL import Image
import os

# --- 1. æˆå¯¹æ•°æ®åŠ è½½å™¨ ---
class EmotionDataset(Dataset):
    def __init__(self, root_A, root_B, transform=None):
        # è¯»å–ç¬‘è„¸(A)å’Œå“­è„¸(B)çš„æ–‡ä»¶è·¯å¾„
        self.smile_images = sorted([os.path.join(root_A, f) for f in os.listdir(root_A) if f.endswith(('.png', '.jpg', '.jpeg'))])
        self.cry_images = sorted([os.path.join(root_B, f) for f in os.listdir(root_B) if f.endswith(('.png', '.jpg', '.jpeg'))])
        self.transform = transform
        
        if len(self.smile_images) != len(self.cry_images):
            print(f"âš ï¸ è­¦å‘Šï¼šç¬‘è„¸({len(self.smile_images)})ä¸å“­è„¸({len(self.cry_images)})æ•°é‡ä¸ä¸€è‡´ï¼è¯·æ£€æŸ¥æ–‡ä»¶åæ˜¯å¦å¯¹åº”ã€‚")

    def __len__(self):
        return min(len(self.smile_images), len(self.cry_images))

    def __getitem__(self, idx):
        smile = Image.open(self.smile_images[idx]).convert('RGB')
        cry = Image.open(self.cry_images[idx]).convert('RGB')
        if self.transform:
            smile = self.transform(smile)
            cry = self.transform(cry)
        return smile, cry

# --- 2. æ ¸å¿ƒè¿è¡Œæµç¨‹ ---
def run_emotion_bridge():
    device = "cuda" if torch.cuda.is_available() else "cpu"
    img_size = 64
    epochs = 500  # æ•°æ®é‡å°æ—¶ï¼Œå¢åŠ è®­ç»ƒè½®æ•°æœ‰åŠ©äºæ¨¡å‹â€œå¼ºè¡Œè®°ä½â€è¡¨æƒ…ç‰¹å¾
    
    # ã€å…³é”®ç»“æ„ã€‘in_channels=6 
    # (3é€šé“ç”¨äºæ¥æ”¶å¸¦å™ªå£°çš„å›¾ + 3é€šé“ç”¨äºæ¥æ”¶åŸå§‹ç¬‘è„¸ä½œä¸ºæ¡ä»¶å¼•å¯¼)
    model = UNet2DModel(
        sample_size=img_size,
        in_channels=6, 
        out_channels=3,
        layers_per_block=2,
        block_out_channels=(64, 128, 256, 256),
        down_block_types=("DownBlock2D", "DownBlock2D", "AttnDownBlock2D", "DownBlock2D"),
        up_block_types=("UpBlock2D", "AttnUpBlock2D", "UpBlock2D", "UpBlock2D"),
    ).to(device)

    optimizer = torch.optim.AdamW(model.parameters(), lr=1e-4)
    noise_scheduler = DDPMScheduler(num_train_timesteps=1000)

    # æ•°æ®é¢„å¤„ç†
    tf = transforms.Compose([
        transforms.Resize((img_size, img_size)),
        transforms.ToTensor(),
        transforms.Normalize([0.5], [0.5])
    ])

    # è·¯å¾„æ£€æŸ¥
    path_a = './data/smile_to_cry/A'
    path_b = './data/smile_to_cry/B'
    if not (os.path.exists(path_a) and os.path.exists(path_b)):
        print(f"âŒ é”™è¯¯ï¼šæœªæ‰¾åˆ°æ•°æ®æ–‡ä»¶å¤¹ï¼è¯·ç¡®ä¿è·¯å¾„å­˜åœ¨ï¼š\n{path_a}\n{path_b}")
        return

    dataset = EmotionDataset(path_a, path_b, transform=tf)
    loader = DataLoader(dataset, batch_size=4, shuffle=True)

    # --- ç¬¬ä¸€éƒ¨åˆ†ï¼šè®­ç»ƒ ---
    print(f"ğŸš€ æ­£åœ¨è®¾å¤‡ {device} ä¸Šå¯åŠ¨â€˜ç¬‘è½¬å“­â€™å¼ºåŒ–è®­ç»ƒ...")
    model.train()
    for epoch in range(epochs):
        loop = tqdm(loader, leave=False)
        for smiles, cries in loop:
            smiles, cries = smiles.to(device), cries.to(device)
            
            # å¯¹ç›®æ ‡ï¼ˆå“­è„¸ï¼‰æ·»åŠ å™ªå£°
            noise = torch.randn_like(cries)
            timesteps = torch.randint(0, 1000, (cries.shape[0],), device=device).long()
            noisy_cries = noise_scheduler.add_noise(cries, noise, timesteps)
            
            # ã€æ ¸å¿ƒæ‹¼æ¥ã€‘å°†å™ªå£°ç›®æ ‡å’Œç¬‘è„¸æ¡ä»¶æ‹¼æˆ 6 é€šé“
            input_combined = torch.cat([noisy_cries, smiles], dim=1)
            
            # é¢„æµ‹å™ªå£°
            prediction = model(input_combined, timesteps).sample
            loss = nn.functional.mse_loss(prediction, noise)
            
            loss.backward()
            optimizer.step()
            optimizer.zero_grad()
            loop.set_description(f"Epoch {epoch}")
            loop.set_postfix(loss=loss.item())

    # ä¿å­˜æ¨¡å‹
    torch.save(model.state_dict(), "smile_to_cry_bridge.pth")
    print("âœ… æƒé‡å·²ä¿å­˜ä¸º smile_to_cry_bridge.pth")

    # --- ç¬¬äºŒéƒ¨åˆ†ï¼šå³æ—¶ç”Ÿæˆæµ‹è¯• ---
    print("ğŸ¨ æ­£åœ¨éªŒè¯è½¬æ¢æ•ˆæœ...")
    model.eval()
    # ä½¿ç”¨ DDIM é‡‡æ ·å™¨åŠ é€Ÿç”Ÿæˆ
    ddim_scheduler = DDIMScheduler(num_train_timesteps=1000)
    ddim_scheduler.set_timesteps(50)

    with torch.no_grad():
        # ä»æ•°æ®é›†ä¸­å–å‡ºä¸€ç»„ç¬‘è„¸è¿›è¡Œæµ‹è¯•
        test_smiles, _ = next(iter(loader))
        test_smiles = test_smiles.to(device)
        
        # ä»çº¯å™ªå£°å¼€å§‹â€œæ´—å›¾â€
        image = torch.randn_like(test_smiles)
        
        for t in tqdm(ddim_scheduler.timesteps, desc="è¡¨æƒ…è½¬æ¢ä¸­"):
            # æ¯ä¸€æ­¥éƒ½è¦å‚è€ƒåŸå§‹ç¬‘è„¸
            combined_input = torch.cat([image, test_smiles], dim=1)
            model_output = model(combined_input, t).sample
            image = ddim_scheduler.step(model_output, t, image).prev_sample

        # ç»“æœå¤„ç†å¹¶ä¿å­˜ï¼šä¸Šé¢æ˜¯è¾“å…¥çš„ç¬‘è„¸ï¼Œä¸‹é¢æ˜¯ç”Ÿæˆçš„å“­è„¸
        result = torch.cat([test_smiles, image], dim=0)
        result = (result / 2 + 0.5).clamp(0, 1)
        save_image(result, "final_conversion_test.png", nrow=4)
        print("ğŸ‰ è½¬æ¢æµ‹è¯•å®Œæˆï¼è¯·æŸ¥çœ‹ conversion_result.png æŸ¥çœ‹å¯¹æ¯”æ•ˆæœã€‚")

if __name__ == "__main__":
    run_emotion_bridge()
